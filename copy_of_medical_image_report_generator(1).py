# -*- coding: utf-8 -*-
"""Copy_of_Medical_Image_Report_Generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13yGlJa0uqXpLthnuQg1vE0IKRPRNl_Lm
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
raddar_chest_xrays_indiana_university_path = kagglehub.dataset_download('raddar/chest-xrays-indiana-university')

print('Data source import complete.')

import os
for i in os.walk("/kaggle/input/chest-xrays-indiana-university"):
    print(i)
    break

import kagglehub
from kagglehub import KaggleDatasetAdapter

# Set the path to the file you'd like to load
file_path = "indiana_reports.csv"

# Load the latest version
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "raddar/chest-xrays-indiana-university",
  file_path,
  # Provide any additional arguments like
  # sql_query or pandas_kwargs. See the
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)

print("First 5 records:", df.head())

# Load the data using kagglehub.load_dataset as it seems more reliable in this environment.
import pandas as pd
import kagglehub
from kagglehub import KaggleDatasetAdapter

# Set the path to the file you'd like to load
file_path = "indiana_reports.csv"

# Load the latest version
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "raddar/chest-xrays-indiana-university",
  file_path,
  # Provide any additional arguments like
  # sql_query or pandas_kwargs. See the
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)

# Display the first few rows to confirm it loaded correctly
display(df.head())

df.dropna(subset={'findings','impression'},inplace=True)

df['report']=df['findings']+' '+df['impression']

import re
def f(s):
    s=s.lower()
    s = re.sub(r"[^a-z.,]", " ", s)
    s = re.sub(r"\.+",".",s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

df.report=df.report.apply(f)

df.report.iloc[0]

import kagglehub
from kagglehub import KaggleDatasetAdapter
import pandas as pd

# Set the path to the file you'd like to load
file_path = "indiana_projections.csv"

# Load the latest version
proj_df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "raddar/chest-xrays-indiana-university",
  file_path,
  # Provide any additional arguments like
  # sql_query or pandas_kwargs. See the
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)

display(proj_df)

df=pd.merge(df,proj_df,how='inner',on='uid')

df=df[df['projection']=='Frontal']
df

import nltk
from collections import Counter

# Download the necessary NLTK data if not already present
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')


text=" ".join(df['report'])
tokens=nltk.word_tokenize(text)
counterr=Counter(tokens)

import nltk
nltk.download('punkt_tab')

word2idx = {"<PAD>": 0, "<UNK>": 1, "<SOS>": 2, "<EOS>": 3}
j = 4
for i in counterr.keys():
    word2idx[i]=j
    j+=1

class Tokenizer:
    def __init__(self, word2idx):
        self.word2idx = word2idx
        self.idx2word = {v: k for k, v in word2idx.items()}
        self.pad_token = "<PAD>"
        self.unk_token = "<UNK>"
        self.sos_token = "<SOS>"
        self.eos_token = "<EOS>"

        self.pad_id = self.word2idx[self.pad_token]
        self.unk_id = self.word2idx[self.unk_token]
        self.sos_id = self.word2idx[self.sos_token]
        self.eos_id = self.word2idx[self.eos_token]

        self.vocab_size = len(word2idx)

    def encode(self, sentence, add_special_tokens=True):
        tokens = nltk.word_tokenize(sentence.lower())
        token_ids = [self.word2idx.get(token, self.unk_id) for token in tokens]

        if add_special_tokens:
            token_ids = [self.sos_id] + token_ids + [self.eos_id]
        return token_ids

    def decode(self, token_ids, skip_special_tokens=True):
        words = []
        for idx in token_ids:
            word = self.idx2word.get(idx, self.unk_token)
            if skip_special_tokens and word in {self.pad_token, self.sos_token, self.eos_token}:
                continue
            words.append(word)
        return " ".join(words)

tokenizer = Tokenizer(word2idx)

sentence = "lungs are clear"
encoded = tokenizer.encode(sentence)
print(encoded)  # e.g. [2, 123, 45, 3]

decoded = tokenizer.decode(encoded)
print(decoded)  # 'lungs are clear'

df["sequence"]=df["report"].apply(tokenizer.encode)
df.head(2)

row=df.iloc[0]
tokenizer.decode(row.sequence)

lengths=[len(i) for i in df['sequence']]

import seaborn as sns
sns.violinplot(lengths)

sum([i>100 for i in lengths])

seq_len=100
def pad_and_trim(seq):
    # Truncate if longer
    if len(seq) > seq_len:
        seq = seq[:seq_len-1] + [tokenizer.eos_id]
        return seq
    else:
        return seq + [tokenizer.pad_id] * (seq_len - len(seq))
df.sequence=df.sequence.apply(pad_and_trim)

df['path']="/kaggle/input/chest-xrays-indiana-university/images/images_normalized/"+df['filename']
df.head(2)

import matplotlib.pyplot as plt
from PIL import Image
def visualize(img,report,generated_report=None):
    if type(img)==str:
        img=Image.open(img)
    plt.imshow(img,cmap="gray")
    plt.axis('off')
    plt.show()
    print("Report :",report)
    if generated_report:
        print("Generated Report :",generated_report)

row=df.iloc[0]
visualize(row['path'],row['report'])

from sklearn.model_selection import train_test_split
train_df,temp=train_test_split(df,test_size=0.2)
test_df,val_df=train_test_split(temp,test_size=0.5)
print(len(train_df),len(test_df),len(val_df))

"""================================="""

pip download rouge-score

!pip install bert_score

!pip install rouge_score

import numpy as np
import pandas as pd
import os
import re
import nltk
import math
from collections import Counter
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from PIL import Image
import seaborn as sns
import textwrap
import random
from tqdm import tqdm
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from bert_score import score as bert_scorer
import copy

# Download NLTK data
nltk.download('punkt')
nltk.download('wordnet')

# Dataset Loading
def load_dataset(data_dir, reports_csv, projections_csv=None):
    df = pd.read_csv(os.path.join(data_dir, reports_csv))
    if projections_csv:
        proj_df = pd.read_csv(os.path.join(data_dir, projections_csv))
        df = pd.merge(df, proj_df, how='inner', on='uid')
        df = df[df['projection'] == 'Frontal']  # Filter for frontal views (modify if needed)
    df.dropna(subset=['findings', 'impression'], inplace=True)
    df['report'] = df['findings'] + ' ' + df['impression']
    return df

# Text Preprocessing
def clean_text(s):
    s = s.lower()
    s = re.sub(r"[^a-z.,]", " ", s)
    s = re.sub(r"\.+", ".", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# Build Vocabulary
def build_vocabulary(reports):
    text = " ".join(reports)
    tokens = nltk.word_tokenize(text)
    counter = Counter(tokens)
    word2idx = {"<PAD>": 0, "<UNK>": 1, "<SOS>": 2, "<EOS>": 3}
    j = 4
    for word in counter.keys():
        word2idx[word] = j
        j += 1
    return word2idx

# Tokenizer Class
class Tokenizer:
    def __init__(self, word2idx):
        self.word2idx = word2idx
        self.idx2word = {v: k for k, v in word2idx.items()}
        self.pad_token = "<PAD>"
        self.unk_token = "<UNK>"
        self.sos_token = "<SOS>"
        self.eos_token = "<EOS>"
        self.pad_id = self.word2idx[self.pad_token]
        self.unk_id = self.word2idx[self.unk_token]
        self.sos_id = self.word2idx[self.sos_token]
        self.eos_id = self.word2idx[self.eos_token]
        self.vocab_size = len(word2idx)

def encode(self, sentence, add_special_tokens=True):
        tokens = nltk.word_tokenize(sentence.lower())
        token_ids = [self.word2idx.get(token, self.unk_id) for token in tokens]
        if add_special_tokens:
            token_ids = [self.sos_id] + token_ids + [self.eos_id]
        return token_ids

def decode(self, token_ids, skip_special_tokens=True):
        words = []
        for idx in token_ids:
            word = self.idx2word.get(idx, self.unk_token)
            if skip_special_tokens and word in {self.pad_token, self.sos_token, self.eos_token}:
                continue
            words.append(word)
        return " ".join(words)

# Image Preprocessing
img_size = (512, 512)
train_transforms = transforms.Compose([
    transforms.Resize(img_size),
    transforms.RandomRotation(degrees=10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),
    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])
image_transforms = transforms.Compose([
    transforms.Resize(img_size),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

# Custom Dataset
class MedicalImageDataset(Dataset):
    def __init__(self, image_paths, captions_seq, transform=None):
        self.image_paths = image_paths
        self.captions_seq = captions_seq
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert("RGB")  # Convert to RGB for compatibility
        if self.transform:
            image = self.transform(image)
        caption_seq = self.captions_seq[idx]
        return image, torch.tensor(caption_seq, dtype=torch.long)

# Image Encoder
class ImageEncoder(nn.Module):
    def __init__(self, embed_dim=512):
        super(ImageEncoder, self).__init__()
        effnet = models.efficientnet_b4(pretrained=True)
        self.backbone = effnet.features
        for param in self.backbone[:-2].parameters():
            param.requires_grad = False
        for param in self.backbone[-2:].parameters():
            param.requires_grad = True
        self.pool = nn.AdaptiveAvgPool2d((7, 7))
        self.flatten = nn.Flatten(2)
        self.transpose = lambda x: x.permute(0, 2, 1)
        self.project = nn.Linear(1792, embed_dim)

    def forward(self, x):
        x = self.backbone(x)
        x = self.pool(x)
        x = self.flatten(x)
        x = self.transpose(x)
        x = self.project(x)
        return x

# Transformer Encoder Block
class TransformerEncoderBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        attn_output, _ = self.self_attn(x, x, x)
        x = self.norm1(x + self.dropout(attn_output))
        ff_output = self.ff(x)
        x = self.norm2(x + self.dropout(ff_output))
        return x

class TransformerDecoderBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.norm3 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, tgt_mask=None):
        # Masked self-attention
        _x = self.norm1(x + self.dropout(self.self_attn(x, x, x, attn_mask=tgt_mask)[0]))
        # Cross-attention
        _x = self.norm2(_x + self.dropout(self.cross_attn(_x, enc_out, enc_out)[0]))
        # Feedforward
        out = self.norm3(_x + self.dropout(self.ff(_x)))
        return out

# Transformer Decoder Block
class TransformerDecoderBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.norm3 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, tgt_mask=None):
        _x = self.norm1(x + self.dropout(self.self_attn(x, x, x, attn_mask=tgt_mask)[0]))
        _x = self.norm2(_x + self.dropout(self.cross_attn(_x, enc_out, enc_out)[0]))
        out = self.norm3(_x + self.dropout(self.ff(_x)))
        return out

# Positional Embedding
class PositionalEmbedding(nn.Module):
    def __init__(self, vocab_size, max_len, embed_dim):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.position_embedding = nn.Embedding(max_len, embed_dim)

    def forward(self, x):
        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)
        pos_embed = self.position_embedding(positions)
        tok_embed = self.token_embedding(x)
        return tok_embed + pos_embed

# Caption Decoder
class CaptionDecoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, ff_dim, num_heads, max_len, num_layers):
        super().__init__()
        self.pos_embed = PositionalEmbedding(vocab_size, max_len, embed_dim)
        self.dec_layers = nn.ModuleList([
            TransformerDecoderBlock(embed_dim, num_heads, ff_dim)
            for _ in range(num_layers)
        ])
        self.output_proj = nn.Linear(embed_dim, vocab_size)

    def make_causal_mask(self, size):
        return torch.triu(torch.ones(size, size), diagonal=1).bool()

    def forward(self, tgt, enc_out):
        x = self.pos_embed(tgt)
        B, T, _ = x.shape
        mask = self.make_causal_mask(T).to(x.device)
        for layer in self.dec_layers:
            x = layer(x, enc_out, tgt_mask=mask)
        logits = self.output_proj(x)
        return logits

# Full Model
import math
class ImageCaptioningModel(nn.Module):
    def __init__(self, cnn_encoder, transformer_encoder, decoder, tokenizer):
        super().__init__()
        self.cnn_encoder = cnn_encoder
        self.transformer_encoder = transformer_encoder
        self.decoder = decoder
        self.tokenizer = tokenizer

    def forward(self, images, captions):
        img_features = self.cnn_encoder(images)
        encoded_img = self.transformer_encoder(img_features)
        logits = self.decoder(captions, encoded_img)
        return logits

    def generate(self, image, max_length=100, beam_width=3, device='cuda', length_penalty=0.7):
        self.eval()
        with torch.no_grad():
            image = image.unsqueeze(0).to(device)
            img_features = self.cnn_encoder(image)
            encoded_img = self.transformer_encoder(img_features)
            beam = [([self.tokenizer.sos_id], 0.0)]
            for _ in range(max_length):
                candidates = []
                for seq, score in beam:
                    if seq[-1] == self.tokenizer.eos_id:
                        candidates.append((seq, score))
                        continue
                    input_ids = torch.tensor(seq).unsqueeze(0).to(device)
                    logits = self.decoder(input_ids, encoded_img)
                    probs = torch.softmax(logits[0, -1, :], dim=-1)
                    topk_probs, topk_ids = probs.topk(beam_width)
                    for prob, idx in zip(topk_probs, topk_ids):
                        new_seq = seq + [idx.item()]
                        new_score = score + math.log(prob.item() + 1e-12)
                        candidates.append((new_seq, new_score))
                beam = sorted(candidates, key=lambda x: x[1] / ((len(x[0]) ** length_penalty) if length_penalty > 0 else 1), reverse=True)[:beam_width]
                if all(seq[-1] == self.tokenizer.eos_id for seq, _ in beam):
                    break
            best_seq = beam[0][0]
            return self.tokenizer.decode(best_seq, skip_special_tokens=True)

# Loss Function
def caption_loss_fn(logits, targets, pad_token_id):
    logits = logits.view(-1, logits.size(-1))
    targets = targets.reshape(-1)
    loss = F.cross_entropy(logits, targets, ignore_index=pad_token_id, label_smoothing=0.1)
    return loss

# Training Function
def train_one_epoch(model, dataloader, optimizer, pad_token_id, device):
    model.train()
    total_loss = 0
    for images, captions in tqdm(dataloader):
        images, captions = images.to(device), captions.to(device)
        inputs = captions[:, :-1]
        targets = captions[:, 1:]
        optimizer.zero_grad()
        logits = model(images, inputs)
        loss = caption_loss_fn(logits, targets, pad_token_id)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

# Evaluation Function
def evaluate_loss(model, dataloader, pad_token_id, device):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for images, captions in tqdm(dataloader):
            images, captions = images.to(device), captions.to(device)
            inputs = captions[:, :-1]
            targets = captions[:, 1:]
            logits = model(images, inputs)
            loss = caption_loss_fn(logits, targets, pad_token_id)
            total_loss += loss.item()
    return total_loss / len(dataloader)

# Metrics Evaluation
def evaluate_model(model, dataloader, tokenizer, device):
    model.eval()
    references = []
    hypotheses = []
    print("Generating predictions on the test set...")
    with torch.no_grad():
        for images, captions_seq in tqdm(dataloader):
            images = images.to(device)
            for i in range(images.size(0)):
                image = images[i]
                generated_caption = model.generate(image, max_length=100, device=device)
                hypotheses.append(generated_caption)
            for seq in captions_seq:
                ref_caption = tokenizer.decode(seq.tolist(), skip_special_tokens=True)
                references.append(ref_caption)
    print(f"\nGenerated {len(hypotheses)} hypotheses.")
    bleu_scores = {'bleu_1': 0, 'bleu_2': 0, 'bleu_3': 0, 'bleu_4': 0}
    for ref, hyp in zip(references, hypotheses):
        ref_tokens = [nltk.word_tokenize(ref)]
        hyp_tokens = nltk.word_tokenize(hyp)
        bleu_scores['bleu_1'] += sentence_bleu(ref_tokens, hyp_tokens, weights=(1, 0, 0, 0))
        bleu_scores['bleu_2'] += sentence_bleu(ref_tokens, hyp_tokens, weights=(0, 1, 0, 0))
        bleu_scores['bleu_3'] += sentence_bleu(ref_tokens, hyp_tokens, weights=(0, 0, 1, 0))
        bleu_scores['bleu_4'] += sentence_bleu(ref_tokens, hyp_tokens, weights=(0, 0, 0, 1))
    for k in bleu_scores:
        bleu_scores[k] /= len(hypotheses)
    meteor_total = 0
    for ref, hyp in zip(references, hypotheses):
        ref_tokens = nltk.word_tokenize(ref)
        hyp_tokens = nltk.word_tokenize(hyp)
        meteor_total += meteor_score([ref_tokens], hyp_tokens)
    meteor_avg = meteor_total / len(hypotheses)
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}
    for ref, hyp in zip(references, hypotheses):
        scores = scorer.score(ref, hyp)
        rouge_scores['rouge1'] += scores['rouge1'].fmeasure
        rouge_scores['rouge2'] += scores['rouge2'].fmeasure
        rouge_scores['rougeL'] += scores['rougeL'].fmeasure
    for k in rouge_scores:
        rouge_scores[k] /= len(hypotheses)
    P, R, F1 = bert_scorer(hypotheses, references, lang="en", model_type='distilbert-base-uncased', device=device)
    bertscore_avg = F1.mean().item()
    return {**bleu_scores, 'meteor': meteor_avg, **rouge_scores, 'bert_score': bertscore_avg}

# Visualization
def visualize_result(img_path, true_report, generated_report, title="Model Prediction"):
    img = Image.open(img_path).convert("RGB")
    plt.figure(figsize=(12, 6))
    plt.imshow(img, cmap="gray")
    plt.axis('off')
    wrapped_true = textwrap.fill(f"Ground Truth: {true_report}", width=100)
    wrapped_gen = textwrap.fill(f"Generated: {generated_report}", width=100)
    plt.title(title, fontsize=14, pad=20)
    plt.figtext(0.5, 0.01, f"{wrapped_true}\n\n{wrapped_gen}",
                ha="center", va="bottom", fontsize=12, wrap=True, bbox={"facecolor":"white", "alpha":0.7, "pad":5})
    plt.show()

def show_random_test_examples(model, test_df, tokenizer, image_transforms, device, num_examples=3):
    model.eval()
    random_indices = random.sample(range(len(test_df)), num_examples)
    for i, idx in enumerate(random_indices):
        sample_row = test_df.iloc[idx]
        img_path = sample_row['path']
        true_report = sample_row['report']
        image_pil = Image.open(img_path).convert("RGB")
        image_tensor = image_transforms(image_pil).unsqueeze(0).to(device)
        print(f"--- Example {i+1}/{num_examples} ---")
        with torch.no_grad():
            generated_report = model.generate(image_tensor.squeeze(0), max_length=100, device=device)
        visualize_result(img_path, true_report, generated_report, title=f"Test Example {i+1}")

# Main Execution with Early Stopping
def main():
    # Use the already loaded and processed dataframes and tokenizer
    global train_df, val_df, test_df, tokenizer, seq_len

    # Create Datasets and DataLoaders
    train_dataset = MedicalImageDataset(
        image_paths=list(train_df['path']),
        captions_seq=list(train_df['sequence']),
        transform=train_transforms
    )
    val_dataset = MedicalImageDataset(
        image_paths=list(val_df['path']),
        captions_seq=list(val_df['sequence']),
        transform=image_transforms
    )
    test_dataset = MedicalImageDataset(
        image_paths=list(test_df['path']),
        captions_seq=list(test_df['sequence']),
        transform=image_transforms
    )

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, drop_last=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, drop_last=True)

    # Model Parameters
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    embed_dim = 512
    ff_dim = 512
    num_heads = 8
    num_decoder_layers = 4
    max_len = seq_len  # Use the already defined seq_len

    # Initialize Model Components
    cnn_encoder = ImageEncoder(embed_dim=embed_dim).to(device)
    transformer_encoder = TransformerEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim).to(device)
    decoder = CaptionDecoder(
        vocab_size=tokenizer.vocab_size,
        embed_dim=embed_dim,
        ff_dim=ff_dim,
        num_heads=num_heads,
        max_len=max_len,
        num_layers=num_decoder_layers
    ).to(device)
    model = ImageCaptioningModel(cnn_encoder, transformer_encoder, decoder, tokenizer).to(device)

    # Optimizer and Scheduler
    cnn_params = list(model.cnn_encoder.backbone[-2:].parameters())
    transformer_params = list(model.transformer_encoder.parameters()) + list(model.decoder.parameters())
    optimizer = torch.optim.Adam([
        {'params': transformer_params, 'lr': 1e-4},
        {'params': cnn_params, 'lr': 1e-5}
    ])
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)

    # Training Loop with Early Stopping
    best_val_loss = float('inf')
    best_model = copy.deepcopy(model.state_dict())
    patience = 5   # stop if no improvement for N epochs
    wait = 0       # counter for patience

    for epoch in range(60):
        train_loss = train_one_epoch(model, train_loader, optimizer, tokenizer.pad_id, device)
        val_loss = evaluate_loss(model, val_loader, tokenizer.pad_id, device)
        scheduler.step(val_loss)

        if val_loss < best_val_loss:
            print("Saving best model...")
            best_val_loss = val_loss
            best_model = copy.deepcopy(model.state_dict())
            wait = 0  # reset patience counter
        else:
            wait += 1
            print(f"No improvement. Early stopping counter: {wait}/{patience}")

        print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

        if wait >= patience:
            print("\nEarly stopping triggered. Training stopped.")
            break

    # Load Best Model
    model.load_state_dict(best_model)
    torch.save(model.state_dict(), "medical_report_model.pt")

    # Evaluate on Test Set
    test_metrics = evaluate_model(model, test_loader, tokenizer, device)
    print("\n--- Test Set Evaluation Metrics ---")
    for metric, value in test_metrics.items():
        print(f"{metric.upper():<12}: {value:.4f}")

    # Visualize Random Test Examples
    show_random_test_examples(model, test_df, tokenizer, image_transforms, device, num_examples=3)


if __name__ == "__main__":
    main()

"""===================================="""

from google.colab import files
files.download("medical_report_model.pt")